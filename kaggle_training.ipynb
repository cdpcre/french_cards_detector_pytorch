{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# French Cards Detector - Kaggle Training\n",
    "\n",
    "Questo notebook prepara l'ambiente ed esegue il training del modello YOLOv11 personalizzato."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Installazione Dipendenze\n",
    "!pip install ultralytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Copia del Dataset in /kaggle/working per velocit\u00e0 I/O\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "source_dir = '/kaggle/input/playing-cards-unified-yolo'\n",
    "dest_dir = '/kaggle/working/playing-cards-unified-yolo'\n",
    "\n",
    "if not os.path.exists(dest_dir):\n",
    "    print(f\"Coping dataset from {source_dir} to {dest_dir}...\")\n",
    "    shutil.copytree(source_dir, dest_dir)\n",
    "    print(\"Copy complete.\")\n",
    "else:\n",
    "    print(f\"Dataset already exists at {dest_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Definizione Classi e Funzioni di Training\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import yaml\n",
    "import random\n",
    "import numpy as np\n",
    "import argparse\n",
    "from pathlib import Path\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.transforms import v2\n",
    "from torchvision import tv_tensors\n",
    "from ultralytics import YOLO\n",
    "from ultralytics.utils.loss import v8DetectionLoss\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Helper Functions\n",
    "def xywhn2xyxy(x, w=640, h=640):\n",
    "    # Convert nx4 boxes from [x, y, w, h] normalized to [x1, y1, x2, y2] absolute\n",
    "    y = x.clone() if isinstance(x, torch.Tensor) else np.copy(x)\n",
    "    y[..., 0] = w * (x[..., 0] - x[..., 2] / 2)  # top left x\n",
    "    y[..., 1] = h * (x[..., 1] - x[..., 3] / 2)  # top left y\n",
    "    y[..., 2] = w * (x[..., 0] + x[..., 2] / 2)  # bottom right x\n",
    "    y[..., 3] = h * (x[..., 1] + x[..., 3] / 2)  # bottom right y\n",
    "    return y\n",
    "\n",
    "def xyxy2xywhn(x, w=640, h=640):\n",
    "    # Convert nx4 boxes from [x1, y1, x2, y2] absolute to [x, y, w, h] normalized\n",
    "    y = x.clone() if isinstance(x, torch.Tensor) else np.copy(x)\n",
    "    y[..., 0] = ((x[..., 0] + x[..., 2]) / 2) / w  # x center\n",
    "    y[..., 1] = ((x[..., 1] + x[..., 3]) / 2) / h  # y center\n",
    "    y[..., 2] = (x[..., 2] - x[..., 0]) / w  # width\n",
    "    y[..., 3] = (x[..., 3] - x[..., 1]) / h  # height\n",
    "    return y\n",
    "\n",
    "class YOLODataset(Dataset):\n",
    "    def __init__(self, yaml_path, split='train', img_size=640, transform=None, mosaic_prob=0.0):\n",
    "        self.img_size = img_size\n",
    "        self.transform = transform\n",
    "        self.mosaic_prob = mosaic_prob\n",
    "        self.split = split\n",
    "        \n",
    "        # Load yaml\n",
    "        with open(yaml_path, 'r') as f:\n",
    "            self.data_cfg = yaml.safe_load(f)\n",
    "            \n",
    "        self.root = Path(yaml_path).parent\n",
    "        # Handle path relative to yaml or absolute\n",
    "        if os.path.isabs(self.data_cfg[split]):\n",
    "             img_dir = Path(self.data_cfg[split])\n",
    "        else:\n",
    "             img_dir = self.root / self.data_cfg[split]\n",
    "             \n",
    "        self.img_paths = sorted(list(img_dir.glob(\"*.jpg\")) + list(img_dir.glob(\"*.png\")))\n",
    "        \n",
    "        # Cache labels\n",
    "        self.labels = []\n",
    "        for img_path in self.img_paths:\n",
    "            label_path = img_path.parent.parent.parent / 'labels' / img_path.parent.name / (img_path.stem + \".txt\")\n",
    "            if label_path.exists():\n",
    "                with open(label_path, 'r') as f:\n",
    "                    l = [x.split() for x in f.read().strip().splitlines() if len(x)]\n",
    "                    l = np.array(l, dtype=np.float32) if len(l) else np.zeros((0, 5), dtype=np.float32)\n",
    "                self.labels.append(l)\n",
    "            else:\n",
    "                self.labels.append(np.zeros((0, 5), dtype=np.float32))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)\n",
    "\n",
    "    def load_image_and_label(self, index):\n",
    "        img_path = self.img_paths[index]\n",
    "        img = cv2.imread(str(img_path))\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        h, w = img.shape[:2]\n",
    "        \n",
    "        label = self.labels[index].copy()\n",
    "        bboxes = label[:, 1:] if len(label) > 0 else np.zeros((0, 4), dtype=np.float32)\n",
    "        cls = label[:, 0] if len(label) > 0 else np.zeros((0,), dtype=np.float32)\n",
    "        \n",
    "        # Convert xywhn to xyxy absolute\n",
    "        if len(bboxes) > 0:\n",
    "            bboxes = xywhn2xyxy(bboxes, w, h)\n",
    "            \n",
    "        return img, bboxes, cls\n",
    "\n",
    "    def load_mosaic(self, index):\n",
    "        # YOLO Mosaic implementation\n",
    "        s = self.img_size\n",
    "        xc = int(random.uniform(-s // 2, 2 * s + s // 2))\n",
    "        yc = int(random.uniform(-s // 2, 2 * s + s // 2))\n",
    "        \n",
    "        indices = [index] + random.choices(range(len(self)), k=3)\n",
    "        random.shuffle(indices)\n",
    "        \n",
    "        result_img = np.full((s * 2, s * 2, 3), 114, dtype=np.uint8)\n",
    "        result_bboxes = []\n",
    "        result_cls = []\n",
    "        \n",
    "        for i, idx in enumerate(indices):\n",
    "            img, bboxes, cls = self.load_image_and_label(idx)\n",
    "            h, w = img.shape[:2]\n",
    "            \n",
    "            # Define placement coordinates (unclamped)\n",
    "            if i == 0:  # top left\n",
    "                x1a, y1a, x2a, y2a = xc - w, yc - h, xc, yc\n",
    "            elif i == 1:  # top right\n",
    "                x1a, y1a, x2a, y2a = xc, yc - h, xc + w, yc\n",
    "            elif i == 2:  # bottom left\n",
    "                x1a, y1a, x2a, y2a = xc - w, yc, xc, yc + h\n",
    "            elif i == 3:  # bottom right\n",
    "                x1a, y1a, x2a, y2a = xc, yc, xc + w, yc + h\n",
    "\n",
    "            # Clamp to canvas\n",
    "            x1a_c = max(0, min(x1a, 2 * s))\n",
    "            x2a_c = max(0, min(x2a, 2 * s))\n",
    "            y1a_c = max(0, min(y1a, 2 * s))\n",
    "            y2a_c = max(0, min(y2a, 2 * s))\n",
    "            \n",
    "            w_c = x2a_c - x1a_c\n",
    "            h_c = y2a_c - y1a_c\n",
    "            \n",
    "            if w_c <= 0 or h_c <= 0:\n",
    "                continue\n",
    "                \n",
    "            # Calculate source coordinates\n",
    "            x1b = x1a_c - x1a\n",
    "            y1b = y1a_c - y1a\n",
    "            x2b = x1b + w_c\n",
    "            y2b = y1b + h_c\n",
    "            \n",
    "            result_img[y1a_c:y2a_c, x1a_c:x2a_c] = img[y1b:y2b, x1b:x2b]\n",
    "            \n",
    "            if len(bboxes) > 0:\n",
    "                # Adjust bboxes\n",
    "                bboxes[:, [0, 2]] += x1a\n",
    "                bboxes[:, [1, 3]] += y1a\n",
    "                result_bboxes.append(bboxes)\n",
    "                result_cls.append(cls)\n",
    "                \n",
    "        if len(result_bboxes) > 0:\n",
    "            result_bboxes = np.concatenate(result_bboxes, 0)\n",
    "            result_cls = np.concatenate(result_cls, 0)\n",
    "            \n",
    "            # Clip boxes to image\n",
    "            np.clip(result_bboxes[:, 0], 0, 2 * s, out=result_bboxes[:, 0])\n",
    "            np.clip(result_bboxes[:, 1], 0, 2 * s, out=result_bboxes[:, 1])\n",
    "            np.clip(result_bboxes[:, 2], 0, 2 * s, out=result_bboxes[:, 2])\n",
    "            np.clip(result_bboxes[:, 3], 0, 2 * s, out=result_bboxes[:, 3])\n",
    "\n            # Filter degenerate boxes\n",
    "            w_box = result_bboxes[:, 2] - result_bboxes[:, 0]\n",
    "            h_box = result_bboxes[:, 3] - result_bboxes[:, 1]\n",
    "            keep = (w_box > 2) & (h_box > 2)\n",
    "            result_bboxes = result_bboxes[keep]\n",
    "            result_cls = result_cls[keep]\n",
    "            \n",
    "        return result_img, result_bboxes, result_cls\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if self.split == 'train' and np.random.rand() < self.mosaic_prob:\n",
    "            img, bboxes, cls = self.load_mosaic(index)\n",
    "        else:\n",
    "            img, bboxes, cls = self.load_image_and_label(index)\n",
    "            \n",
    "        # Prepare for transforms\n",
    "        # Convert to torch tensors\n",
    "        # Ensure CHW format\n",
    "        img = torch.as_tensor(img).permute(2, 0, 1)\n",
    "        img = tv_tensors.Image(img)\n",
    "        \n",
    "        # BoundingBoxes requires shape [N, 4]\n",
    "        if len(bboxes) == 0:\n",
    "            bboxes = torch.zeros((0, 4), dtype=torch.float32)\n",
    "            cls = torch.zeros((0,), dtype=torch.float32)\n",
    "        else:\n",
    "            bboxes = torch.from_numpy(bboxes).float()\n",
    "            cls = torch.from_numpy(cls).float()\n",
    "            \n",
    "        bboxes = tv_tensors.BoundingBoxes(bboxes, format=\"XYXY\", canvas_size=img.shape[-2:])\n",
    "        \n",
    "        if self.transform:\n",
    "            img, bboxes, cls = self.transform(img, bboxes, cls)\n",
    "            \n",
    "        # Normalize image 0-1\n",
    "        img = img.float() / 255.0\n",
    "        \n",
    "        # Convert boxes back to xywhn for YOLO loss\n",
    "        h, w = img.shape[-2:]\n",
    "        if len(bboxes) > 0:\n",
    "            bboxes_norm = xyxy2xywhn(bboxes, w, h)\n",
    "            # Create target tensor [idx, cls, x, y, w, h]\n",
    "            # Note: idx will be added in collate_fn\n",
    "            targets = torch.cat((cls.unsqueeze(1), bboxes_norm), dim=1)\n",
    "        else:\n",
    "            targets = torch.zeros((0, 5))\n",
    "            \n",
    "        return img, targets\n",
    "        \n",
    "    def collate_fn(self, batch):\n",
    "        imgs, targets = zip(*batch)\n",
    "        imgs = torch.stack(imgs, 0)\n",
    "        \n",
    "        # Add batch index to targets\n",
    "        new_targets = []\n",
    "        for i, t in enumerate(targets):\n",
    "            if t.shape[0] > 0:\n",
    "                idx = torch.full((t.shape[0], 1), i)\n",
    "                new_targets.append(torch.cat((idx, t), 1))\n",
    "        \n",
    "        if new_targets:\n",
    "            targets = torch.cat(new_targets, 0)\n",
    "        else:\n",
    "            targets = torch.zeros((0, 6))\n",
    "            \n",
    "        return imgs, targets\n",
    "\n",
    "def train(args):\n",
    "    # Device\n",
    "    device = torch.device(args.device if torch.cuda.is_available() or torch.backends.mps.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Transforms\n",
    "    train_transform = v2.Compose([\n",
    "        v2.Resize((args.imgsz, args.imgsz)),\n",
    "        v2.RandomAffine(degrees=0, translate=(0.1, 0.1), scale=(0.5, 1.5)),\n",
    "        v2.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.7, hue=0.015),\n",
    "    ])\n",
    "\n",
    "    val_transform = v2.Compose([\n",
    "        v2.Resize((args.imgsz, args.imgsz)),\n",
    "    ])\n",
    "    \n",
    "    # Datasets\n",
    "    train_dataset = YOLODataset(args.data, split='train', img_size=args.imgsz, transform=train_transform, mosaic_prob=args.mosaic)\n",
    "    val_dataset = YOLODataset(args.data, split='val', img_size=args.imgsz, transform=val_transform, mosaic_prob=0.0)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=args.batch, shuffle=True, collate_fn=train_dataset.collate_fn, num_workers=args.workers)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=args.batch, shuffle=False, collate_fn=val_dataset.collate_fn, num_workers=args.workers)\n",
    "\n",
    "    print(f\"Train images: {len(train_dataset)}\")\n",
    "    print(f\"Val images: {len(val_dataset)}\")\n",
    "    \n",
    "    # Model\n",
    "    print(f\"Loading model: {args.model}\")\n",
    "    model_wrapper = YOLO(args.model)\n",
    "    model = model_wrapper.model\n",
    "    model.to(device)\n",
    "    \n",
    "    # Force gradients for all parameters\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = True\n",
    "    \n",
    "    # Optimizer\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=args.lr, weight_decay=0.0005)\n",
    "    \n",
    "    # Loss\n",
    "    # Fix for Ultralytics loss expecting attribute access for hyperparameters\n",
    "    if hasattr(model, 'args') and isinstance(model.args, dict):\n",
    "        from types import SimpleNamespace\n",
    "        model.args = SimpleNamespace(**model.args)\n",
    "    \n",
    "    # Ensure hyperparameters exist\n",
    "    if not hasattr(model.args, 'box'): model.args.box = 7.5\n",
    "    if not hasattr(model.args, 'cls'): model.args.cls = 0.5\n",
    "    if not hasattr(model.args, 'dfl'): model.args.dfl = 1.5\n",
    "        \n",
    "    loss_fn = v8DetectionLoss(model_wrapper.model)\n",
    "    \n",
    "    # Training Loop\n",
    "    os.makedirs(args.project, exist_ok=True)\n",
    "    \n",
    "    for epoch in range(args.epochs):\n",
    "        model.train()\n",
    "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{args.epochs}\")\n",
    "        total_loss = 0\n",
    "        \n",
    "        for imgs, targets in pbar:\n",
    "            imgs = imgs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            preds = model(imgs)\n",
    "            \n",
    "            batch_data = {\n",
    "                \"batch_idx\": targets[:, 0],\n",
    "                \"cls\": targets[:, 1].view(-1, 1),\n",
    "                \"bboxes\": targets[:, 2:],\n",
    "                \"device\": device,\n",
    "                \"img\": imgs\n",
    "            }\n",
    "            \n",
    "            loss, loss_items = loss_fn(preds, batch_data)\n",
    "            \n",
    "            # Ensure loss is scalar\n",
    "            if loss.ndim > 0:\n",
    "                loss = loss.sum()\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            pbar.set_postfix({\"loss\": loss.item()})\n",
    "            \n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        print(f\"Epoch {epoch+1} Average Loss: {avg_loss:.4f}\")\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for imgs, targets in val_loader:\n",
    "                imgs = imgs.to(device)\n",
    "                targets = targets.to(device)\n",
    "                preds = model(imgs)\n",
    "                batch_data = {\n",
    "                    \"batch_idx\": targets[:, 0],\n",
    "                    \"cls\": targets[:, 1].view(-1, 1),\n",
    "                    \"bboxes\": targets[:, 2:],\n",
    "                    \"device\": device,\n",
    "                    \"img\": imgs\n",
    "                }\n",
    "                loss, _ = loss_fn(preds, batch_data)\n",
    "                if loss.ndim > 0:\n",
    "                    loss = loss.sum()\n",
    "                val_loss += loss.item()\n",
    "                \n",
    "        print(f\"Val Loss: {val_loss / len(val_loader):.4f}\")\n",
    "        \n",
    "        # Save checkpoint\n",
    "        torch.save(model.state_dict(), f\"{args.project}/custom_yolo_epoch_{epoch+1}.pt\")\n",
    "        \n",
    "    return val_loss / len(val_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7da871",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Configurazione ed Esecuzione Training\n",
    "class Args:\n",
    "    data = '/kaggle/working/playing-cards-unified-yolo/unified/data.yaml'\n",
    "    model = 'yolo11n.pt'\n",
    "    epochs = 50\n",
    "    batch = 16\n",
    "    imgsz = 640\n",
    "    lr = 1e-3\n",
    "    mosaic = 0.5\n",
    "    device = 'cuda' # Usa GPU NVIDIA su Kaggle\n",
    "    project = 'runs/train_custom'\n",
    "    workers = 2\n",
    "\n",
    "args = Args()\n",
    "train(args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}