{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import yaml\n",
    "import random\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.transforms import v2\n",
    "from torchvision import tv_tensors\n",
    "from ultralytics import YOLO\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Check device\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "DATA_YAML = \"datasets/unified/data.yaml\"\n",
    "IMG_SIZE = 640\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 10\n",
    "LEARNING_RATE = 1e-3\n",
    "MOSAIC_PROB = 0.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xywhn2xyxy(x, w=640, h=640):\n",
    "    # Convert nx4 boxes from [x, y, w, h] normalized to [x1, y1, x2, y2] absolute\n",
    "    y = x.clone() if isinstance(x, torch.Tensor) else np.copy(x)\n",
    "    y[..., 0] = w * (x[..., 0] - x[..., 2] / 2)  # top left x\n",
    "    y[..., 1] = h * (x[..., 1] - x[..., 3] / 2)  # top left y\n",
    "    y[..., 2] = w * (x[..., 0] + x[..., 2] / 2)  # bottom right x\n",
    "    y[..., 3] = h * (x[..., 1] + x[..., 3] / 2)  # bottom right y\n",
    "    return y\n",
    "\n",
    "def xyxy2xywhn(x, w=640, h=640):\n",
    "    # Convert nx4 boxes from [x1, y1, x2, y2] absolute to [x, y, w, h] normalized\n",
    "    y = x.clone() if isinstance(x, torch.Tensor) else np.copy(x)\n",
    "    y[..., 0] = ((x[..., 0] + x[..., 2]) / 2) / w  # x center\n",
    "    y[..., 1] = ((x[..., 1] + x[..., 3]) / 2) / h  # y center\n",
    "    y[..., 2] = (x[..., 2] - x[..., 0]) / w  # width\n",
    "    y[..., 3] = (x[..., 3] - x[..., 1]) / h  # height\n",
    "    return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class YOLODataset(Dataset):\n",
    "    def __init__(self, yaml_path, split='train', img_size=640, transform=None, mosaic_prob=0.0):\n",
    "        self.img_size = img_size\n",
    "        self.transform = transform\n",
    "        self.mosaic_prob = mosaic_prob\n",
    "        self.split = split\n",
    "        \n",
    "        # Load yaml\n",
    "        with open(yaml_path, 'r') as f:\n",
    "            self.data_cfg = yaml.safe_load(f)\n",
    "            \n",
    "        self.root = Path(yaml_path).parent\n",
    "        # Handle path relative to yaml or absolute\n",
    "        if os.path.isabs(self.data_cfg[split]):\n",
    "             img_dir = Path(self.data_cfg[split])\n",
    "        else:\n",
    "             img_dir = self.root / self.data_cfg[split]\n",
    "             \n",
    "        self.img_paths = sorted(list(img_dir.glob(\"*.jpg\")) + list(img_dir.glob(\"*.png\")))\n",
    "        \n",
    "        # Cache labels to avoid reading files constantly (optional, but good for speed)\n",
    "        self.labels = []\n",
    "        for img_path in self.img_paths:\n",
    "            label_path = img_path.parent.parent / 'labels' / img_path.parent.name / (img_path.stem + \".txt\")\n",
    "            if label_path.exists():\n",
    "                with open(label_path, 'r') as f:\n",
    "                    l = [x.split() for x in f.read().strip().splitlines() if len(x)]\n",
    "                    l = np.array(l, dtype=np.float32) if len(l) else np.zeros((0, 5), dtype=np.float32)\n",
    "                self.labels.append(l)\n",
    "            else:\n",
    "                self.labels.append(np.zeros((0, 5), dtype=np.float32))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)\n",
    "\n",
    "    def load_image_and_label(self, index):\n",
    "        img_path = self.img_paths[index]\n",
    "        img = cv2.imread(str(img_path))\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        h, w = img.shape[:2]\n",
    "        \n",
    "        label = self.labels[index].copy()\n",
    "        bboxes = label[:, 1:] if len(label) > 0 else np.zeros((0, 4), dtype=np.float32)\n",
    "        cls = label[:, 0] if len(label) > 0 else np.zeros((0,), dtype=np.float32)\n",
    "        \n",
    "        # Convert xywhn to xyxy absolute\n",
    "        if len(bboxes) > 0:\n",
    "            bboxes = xywhn2xyxy(bboxes, w, h)\n",
    "            \n",
    "        return img, bboxes, cls\n",
    "\n",
    "    def load_mosaic(self, index):\n",
    "        # YOLO Mosaic implementation\n",
    "        s = self.img_size\n",
    "        xc = int(random.uniform(-s // 2, 2 * s + s // 2))\n",
    "        yc = int(random.uniform(-s // 2, 2 * s + s // 2))\n",
    "        \n",
    "        indices = [index] + random.choices(range(len(self)), k=3)\n",
    "        random.shuffle(indices)\n",
    "        \n",
    "        result_img = np.full((s * 2, s * 2, 3), 114, dtype=np.uint8)\n",
    "        result_bboxes = []\n",
    "        result_cls = []\n",
    "        \n",
    "        for i, idx in enumerate(indices):\n",
    "            img, bboxes, cls = self.load_image_and_label(idx)\n",
    "            h, w = img.shape[:2]\n",
    "            \n",
    "            # Define placement coordinates (unclamped)\n",
    "            if i == 0:  # top left\n",
    "                x1a, y1a, x2a, y2a = xc - w, yc - h, xc, yc\n",
    "            elif i == 1:  # top right\n",
    "                x1a, y1a, x2a, y2a = xc, yc - h, xc + w, yc\n",
    "            elif i == 2:  # bottom left\n",
    "                x1a, y1a, x2a, y2a = xc - w, yc, xc, yc + h\n",
    "            elif i == 3:  # bottom right\n",
    "                x1a, y1a, x2a, y2a = xc, yc, xc + w, yc + h\n",
    "\n",
    "            # Clamp to canvas\n",
    "            x1a_c = max(0, min(x1a, 2 * s))\n",
    "            x2a_c = max(0, min(x2a, 2 * s))\n",
    "            y1a_c = max(0, min(y1a, 2 * s))\n",
    "            y2a_c = max(0, min(y2a, 2 * s))\n",
    "            \n",
    "            w_c = x2a_c - x1a_c\n",
    "            h_c = y2a_c - y1a_c\n",
    "            \n",
    "            if w_c <= 0 or h_c <= 0:\n",
    "                continue\n",
    "                \n",
    "            # Calculate source coordinates\n",
    "            x1b = x1a_c - x1a\n",
    "            y1b = y1a_c - y1a\n",
    "            x2b = x1b + w_c\n",
    "            y2b = y1b + h_c\n",
    "            \n",
    "            result_img[y1a_c:y2a_c, x1a_c:x2a_c] = img[y1b:y2b, x1b:x2b]\n",
    "            \n",
    "            if len(bboxes) > 0:\n",
    "                # Adjust bboxes\n",
    "                bboxes[:, [0, 2]] += x1a\n",
    "                bboxes[:, [1, 3]] += y1a\n",
    "                result_bboxes.append(bboxes)\n",
    "                result_cls.append(cls)\n",
    "                \n",
    "        if len(result_bboxes) > 0:\n",
    "            result_bboxes = np.concatenate(result_bboxes, 0)\n",
    "            result_cls = np.concatenate(result_cls, 0)\n",
    "            \n",
    "            # Clip boxes to image\n",
    "            np.clip(result_bboxes[:, 0], 0, 2 * s, out=result_bboxes[:, 0])\n",
    "            np.clip(result_bboxes[:, 1], 0, 2 * s, out=result_bboxes[:, 1])\n",
    "            np.clip(result_bboxes[:, 2], 0, 2 * s, out=result_bboxes[:, 2])\n",
    "            np.clip(result_bboxes[:, 3], 0, 2 * s, out=result_bboxes[:, 3])\n",
    "            \n",
    "        return result_img, result_bboxes, result_cls\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if self.split == 'train' and np.random.rand() < self.mosaic_prob:\n",
    "            img, bboxes, cls = self.load_mosaic(index)\n",
    "        else:\n",
    "            img, bboxes, cls = self.load_image_and_label(index)\n",
    "            \n",
    "        # Prepare for transforms\n",
    "        # Convert to torch tensors\n",
    "        # Ensure CHW format\n",
    "        img = torch.from_numpy(img).permute(2, 0, 1)\n",
    "        img = tv_tensors.Image(img)\n",
    "        \n",
    "        # BoundingBoxes requires shape [N, 4]\n",
    "        if len(bboxes) == 0:\n",
    "            bboxes = torch.zeros((0, 4), dtype=torch.float32)\n",
    "            cls = torch.zeros((0,), dtype=torch.float32)\n",
    "        else:\n",
    "            bboxes = torch.from_numpy(bboxes).float()\n",
    "            cls = torch.from_numpy(cls).float()\n",
    "            \n",
    "        bboxes = tv_tensors.BoundingBoxes(bboxes, format=\"XYXY\", canvas_size=img.shape[-2:])\n",
    "        \n",
    "        if self.transform:\n",
    "            img, bboxes, cls = self.transform(img, bboxes, cls)\n",
    "            \n",
    "        # Normalize image 0-1\n",
    "        img = img.float() / 255.0\n",
    "        \n",
    "        # Convert boxes back to xywhn for YOLO loss\n",
    "        h, w = img.shape[-2:]\n",
    "        if len(bboxes) > 0:\n",
    "            bboxes_norm = xyxy2xywhn(bboxes, w, h)\n",
    "            # Create target tensor [idx, cls, x, y, w, h]\n",
    "            # Note: idx will be added in collate_fn\n",
    "            targets = torch.cat((cls.unsqueeze(1), bboxes_norm), dim=1)\n",
    "        else:\n",
    "            targets = torch.zeros((0, 5))\n",
    "            \n",
    "        return img, targets\n",
    "        \n",
    "    def collate_fn(self, batch):\n",
    "        imgs, targets = zip(*batch)\n",
    "        imgs = torch.stack(imgs, 0)\n",
    "        \n",
    "        # Add batch index to targets\n",
    "        new_targets = []\n",
    "        for i, t in enumerate(targets):\n",
    "            if t.shape[0] > 0:\n",
    "                idx = torch.full((t.shape[0], 1), i)\n",
    "                new_targets.append(torch.cat((idx, t), 1))\n",
    "        \n",
    "        if new_targets:\n",
    "            targets = torch.cat(new_targets, 0)\n",
    "        else:\n",
    "            targets = torch.zeros((0, 6))\n",
    "            \n",
    "        return imgs, targets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Transforms using torchvision.transforms.v2\n",
    "train_transform = v2.Compose([\n",
    "    v2.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    v2.RandomHorizontalFlip(p=0.5),\n",
    "    v2.RandomAffine(degrees=0, translate=(0.1, 0.1), scale=(0.5, 1.5)),\n",
    "    v2.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.7, hue=0.015),\n",
    "])\n",
    "\n",
    "val_transform = v2.Compose([\n",
    "    v2.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train images: 64138\n",
      "Val images: 10735\n"
     ]
    }
   ],
   "source": [
    "# Initialize Datasets and Dataloaders\n",
    "train_dataset = YOLODataset(DATA_YAML, split='train', img_size=IMG_SIZE, transform=train_transform, mosaic_prob=MOSAIC_PROB)\n",
    "val_dataset = YOLODataset(DATA_YAML, split='val', img_size=IMG_SIZE, transform=val_transform, mosaic_prob=0.0)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=train_dataset.collate_fn, num_workers=0)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=val_dataset.collate_fn, num_workers=0)\n",
    "\n",
    "print(f\"Train images: {len(train_dataset)}\")\n",
    "print(f\"Val images: {len(val_dataset)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Model\n",
    "# We use Ultralytics to load the architecture and weights, but we will use our own training loop.\n",
    "model_wrapper = YOLO(\"yolo11n.pt\")\n",
    "model = model_wrapper.model\n",
    "model.to(device)\n",
    "\n",
    "# Force gradients for all parameters\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=0.0005)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:  19%|█▊        | 747/4009 [19:36<1:25:35,  1.57s/it, loss=0.00218] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 25\u001b[39m\n\u001b[32m     22\u001b[39m pbar = tqdm(train_loader, desc=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mEPOCHS\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     23\u001b[39m total_loss = \u001b[32m0\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mimgs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpbar\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[43m    \u001b[49m\u001b[43mimgs\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mimgs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/cdpcre/french_cards_detector_pytorch/.venv/lib/python3.12/site-packages/tqdm/std.py:1181\u001b[39m, in \u001b[36mtqdm.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1178\u001b[39m time = \u001b[38;5;28mself\u001b[39m._time\n\u001b[32m   1180\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1181\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1182\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[32m   1183\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[32m   1184\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/cdpcre/french_cards_detector_pytorch/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:732\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    729\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    730\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    731\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m732\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    733\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    734\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    735\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    736\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    737\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    738\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/cdpcre/french_cards_detector_pytorch/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:788\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    786\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    787\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m788\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    789\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    790\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/cdpcre/french_cards_detector_pytorch/.venv/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     50\u001b[39m         data = \u001b[38;5;28mself\u001b[39m.dataset.__getitems__(possibly_batched_index)\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m         data = [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 141\u001b[39m, in \u001b[36mYOLODataset.__getitem__\u001b[39m\u001b[34m(self, index)\u001b[39m\n\u001b[32m    138\u001b[39m bboxes = tv_tensors.BoundingBoxes(bboxes, \u001b[38;5;28mformat\u001b[39m=\u001b[33m\"\u001b[39m\u001b[33mXYXY\u001b[39m\u001b[33m\"\u001b[39m, canvas_size=img.shape[-\u001b[32m2\u001b[39m:])\n\u001b[32m    140\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.transform:\n\u001b[32m--> \u001b[39m\u001b[32m141\u001b[39m     img, bboxes, \u001b[38;5;28mcls\u001b[39m = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbboxes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    143\u001b[39m \u001b[38;5;66;03m# Normalize image 0-1\u001b[39;00m\n\u001b[32m    144\u001b[39m img = img.float() / \u001b[32m255.0\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/cdpcre/french_cards_detector_pytorch/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/cdpcre/french_cards_detector_pytorch/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/cdpcre/french_cards_detector_pytorch/.venv/lib/python3.12/site-packages/torchvision/transforms/v2/_container.py:52\u001b[39m, in \u001b[36mCompose.forward\u001b[39m\u001b[34m(self, *inputs)\u001b[39m\n\u001b[32m     50\u001b[39m needs_unpacking = \u001b[38;5;28mlen\u001b[39m(inputs) > \u001b[32m1\u001b[39m\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m transform \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.transforms:\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m     outputs = \u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     53\u001b[39m     inputs = outputs \u001b[38;5;28;01mif\u001b[39;00m needs_unpacking \u001b[38;5;28;01melse\u001b[39;00m (outputs,)\n\u001b[32m     54\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/cdpcre/french_cards_detector_pytorch/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/cdpcre/french_cards_detector_pytorch/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/cdpcre/french_cards_detector_pytorch/.venv/lib/python3.12/site-packages/torchvision/transforms/v2/_transform.py:190\u001b[39m, in \u001b[36m_RandomApplyTransform.forward\u001b[39m\u001b[34m(self, *inputs)\u001b[39m\n\u001b[32m    184\u001b[39m needs_transform_list = \u001b[38;5;28mself\u001b[39m._needs_transform_list(flat_inputs)\n\u001b[32m    185\u001b[39m params = \u001b[38;5;28mself\u001b[39m.make_params(\n\u001b[32m    186\u001b[39m     [inpt \u001b[38;5;28;01mfor\u001b[39;00m (inpt, needs_transform) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(flat_inputs, needs_transform_list) \u001b[38;5;28;01mif\u001b[39;00m needs_transform]\n\u001b[32m    187\u001b[39m )\n\u001b[32m    189\u001b[39m flat_outputs = [\n\u001b[32m--> \u001b[39m\u001b[32m190\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43minpt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m needs_transform \u001b[38;5;28;01melse\u001b[39;00m inpt\n\u001b[32m    191\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m (inpt, needs_transform) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(flat_inputs, needs_transform_list)\n\u001b[32m    192\u001b[39m ]\n\u001b[32m    194\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m tree_unflatten(flat_outputs, spec)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/cdpcre/french_cards_detector_pytorch/.venv/lib/python3.12/site-packages/torchvision/transforms/v2/_geometry.py:49\u001b[39m, in \u001b[36mRandomHorizontalFlip.transform\u001b[39m\u001b[34m(self, inpt, params)\u001b[39m\n\u001b[32m     48\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtransform\u001b[39m(\u001b[38;5;28mself\u001b[39m, inpt: Any, params: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]) -> Any:\n\u001b[32m---> \u001b[39m\u001b[32m49\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_kernel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhorizontal_flip\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minpt\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/cdpcre/french_cards_detector_pytorch/.venv/lib/python3.12/site-packages/torchvision/transforms/v2/_transform.py:49\u001b[39m, in \u001b[36mTransform._call_kernel\u001b[39m\u001b[34m(self, functional, inpt, *args, **kwargs)\u001b[39m\n\u001b[32m     47\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_call_kernel\u001b[39m(\u001b[38;5;28mself\u001b[39m, functional: Callable, inpt: Any, *args: Any, **kwargs: Any) -> Any:\n\u001b[32m     48\u001b[39m     kernel = _get_kernel(functional, \u001b[38;5;28mtype\u001b[39m(inpt), allow_passthrough=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m---> \u001b[39m\u001b[32m49\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mkernel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minpt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/cdpcre/french_cards_detector_pytorch/.venv/lib/python3.12/site-packages/torchvision/transforms/v2/functional/_utils.py:32\u001b[39m, in \u001b[36m_kernel_tv_tensor_wrapper.<locals>.wrapper\u001b[39m\u001b[34m(inpt, *args, **kwargs)\u001b[39m\n\u001b[32m     21\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(kernel)\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper\u001b[39m(inpt, *args, **kwargs):\n\u001b[32m     23\u001b[39m     \u001b[38;5;66;03m# If you're wondering whether we could / should get rid of this wrapper,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     30\u001b[39m     \u001b[38;5;66;03m# lost after the first operation due to our own __torch_function__\u001b[39;00m\n\u001b[32m     31\u001b[39m     \u001b[38;5;66;03m# logic.\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m     output = \u001b[43mkernel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minpt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mas_subclass\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mTensor\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     33\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m tv_tensors.wrap(output, like=inpt)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/cdpcre/french_cards_detector_pytorch/.venv/lib/python3.12/site-packages/torchvision/transforms/v2/functional/_geometry.py:57\u001b[39m, in \u001b[36mhorizontal_flip_image\u001b[39m\u001b[34m(image)\u001b[39m\n\u001b[32m     54\u001b[39m \u001b[38;5;129m@_register_kernel_internal\u001b[39m(horizontal_flip, torch.Tensor)\n\u001b[32m     55\u001b[39m \u001b[38;5;129m@_register_kernel_internal\u001b[39m(horizontal_flip, tv_tensors.Image)\n\u001b[32m     56\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mhorizontal_flip_image\u001b[39m(image: torch.Tensor) -> torch.Tensor:\n\u001b[32m---> \u001b[39m\u001b[32m57\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimage\u001b[49m\u001b[43m.\u001b[49m\u001b[43mflip\u001b[49m\u001b[43m(\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Training Loop\n",
    "from ultralytics.utils.loss import v8DetectionLoss\n",
    "\n",
    "# We need to initialize the loss function. \n",
    "# Ultralytics loss requires a 'model' object that has 'args' attribute for hyperparameters.\n",
    "# We can use the loaded model_wrapper for this context.\n",
    "\n",
    "# Fix for Ultralytics loss expecting attribute access for hyperparameters\n",
    "if hasattr(model_wrapper.model, 'args') and isinstance(model_wrapper.model.args, dict):\n",
    "    from types import SimpleNamespace\n",
    "    model_wrapper.model.args = SimpleNamespace(**model_wrapper.model.args)\n",
    "\n",
    "# Ensure hyperparameters exist\n",
    "if not hasattr(model_wrapper.model.args, 'box'): model_wrapper.model.args.box = 7.5\n",
    "if not hasattr(model_wrapper.model.args, 'cls'): model_wrapper.model.args.cls = 0.5\n",
    "if not hasattr(model_wrapper.model.args, 'dfl'): model_wrapper.model.args.dfl = 1.5\n",
    "\n",
    "loss_fn = v8DetectionLoss(model_wrapper.model)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS}\")\n",
    "    total_loss = 0\n",
    "    \n",
    "    for imgs, targets in pbar:\n",
    "        imgs = imgs.to(device)\n",
    "        targets = targets.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        # YOLOv8/11 model returns (preds, hidden_states) usually, or just preds depending on mode\n",
    "        # We need to ensure we get what the loss function expects.\n",
    "        # The loss function expects 'preds' which is a list of 3 tensors (for 3 scales)\n",
    "        # and 'batch' which is a dict containing 'batch_idx', 'cls', 'bboxes'.\n",
    "        \n",
    "        preds = model(imgs)\n",
    "        \n",
    "        # Construct batch dictionary for loss function\n",
    "        # targets is [batch_idx, cls, x, y, w, h]\n",
    "        batch_data = {\n",
    "            \"batch_idx\": targets[:, 0],\n",
    "            \"cls\": targets[:, 1].view(-1, 1),\n",
    "            \"bboxes\": targets[:, 2:],\n",
    "            \"device\": device,\n",
    "            \"img\": imgs # needed for some loss calculations (anchors)\n",
    "        }\n",
    "        \n",
    "        loss, loss_items = loss_fn(preds, batch_data)\n",
    "        \n",
    "        # Ensure loss is scalar\n",
    "        if loss.ndim > 0:\n",
    "            loss = loss.sum()\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        pbar.set_postfix({\"loss\": loss.item()})\n",
    "        \n",
    "    print(f\"Epoch {epoch+1} Average Loss: {total_loss / len(train_loader):.4f}\")\n",
    "    \n",
    "    # Validation (Simplified - just loss for now)\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for imgs, targets in val_loader:\n",
    "            imgs = imgs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            preds = model(imgs)\n",
    "            batch_data = {\n",
    "                \"batch_idx\": targets[:, 0],\n",
    "                \"cls\": targets[:, 1].view(-1, 1),\n",
    "                \"bboxes\": targets[:, 2:],\n",
    "                \"device\": device,\n",
    "                \"img\": imgs\n",
    "            }\n",
    "            loss, _ = loss_fn(preds, batch_data)\n",
    "            val_loss += loss.item()\n",
    "            \n",
    "    print(f\"Val Loss: {val_loss / len(val_loader):.4f}\")\n",
    "    \n",
    "    # Save checkpoint\n",
    "    torch.save(model.state_dict(), f\"runs/train/custom_yolo_epoch_{epoch+1}.pt\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
